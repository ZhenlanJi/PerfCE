{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import (\n",
    "    abspath,\n",
    "    dirname,\n",
    ")\n",
    "from prometheus_api_client import (\n",
    "    PrometheusConnect,\n",
    "    MetricSnapshotDataFrame,\n",
    "    MetricRangeDataFrame\n",
    ")\n",
    "import sys\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import pytz\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "prom = PrometheusConnect(url=\"http://192.168.50.113:19090\", disable_ssl=True)\n",
    "\n",
    "query_info_df = pd.read_csv(\"query_info.csv\")\n",
    "\n",
    "# read text file\n",
    "with open(\"query.txt\") as f:\n",
    "    query_list = f.readlines()\n",
    "\n",
    "interval = \"1m\"\n",
    "# replace substring of each line in query_list\n",
    "query_list = list(map(lambda x: x.replace(\n",
    "    '$host', '10.244.2.13:9104'), query_list))\n",
    "query_list = list(map(lambda x: x.replace('$interval', interval), query_list))\n",
    "query_save_path = os.path.join(\n",
    "    dirname(dirname(os.getcwd())), \"data\", \"mysql\", \"test1\", interval)\n",
    "\n",
    "dropped = [\n",
    "    # \"Top_Command_Counters.csv\", \"MySQL_Handlers.csv\",\n",
    "    \"MySQL_Table_Open_Cache_Status__Table_Open_Cache_Hit_Ratio.csv\",\n",
    "    \"MySQL_Connections__Max_Connections.csv\",\n",
    "    'MySQL_Internal_Memory_Overview__InnoDB_Log_Buffer_Size.csv',\n",
    "    'MySQL_Internal_Memory_Overview__Key_Buffer_Size.csv',\n",
    "    'MySQL_Internal_Memory_Overview__Query_Cache_Size.csv',\n",
    "    'MySQL_Open_Files__Open_Files_Limit.csv',\n",
    "    'MySQL_Open_Tables__Table_Open_Cache.csv',\n",
    "    'MySQL_Query_Cache_Memory__Query_Cache_Size.csv',\n",
    "    'MySQL_Table_Definition_Cache__Table_Definitions_Cache_Size.csv',\n",
    "    'MySQL_Thread_Cache__Thread_Cache_Size.csv',\n",
    "    # 'MySQL_Query_Duration.csv',\n",
    "    # 'System_IO_Activity__Page_In.csv',\n",
    "    # 'System_IO_Activity__Page_Out.csv',\n",
    "    # 'System_Memory_Distribution__Free.csv',\n",
    "    # 'System_Memory_Distribution__Total.csv',\n",
    "    # 'System_CPU_Usage_Load.csv',\n",
    "    # 'System_CPU_Usage_Load__Max_Core_Utilization.csv',\n",
    "    # 'System_CPU_Usage_Load__Load_1m.csv',\n",
    "    # 'System_Network_Traffic__Inbound.csv',\n",
    "    # 'System_Network_Traffic__Outbound.csv',\n",
    "    # 'System_Swap_Activity__Swap_In_Reads.csv',\n",
    "    # 'System_Swap_Activity__Swap_Out_Writes.csv',\n",
    "    # 'System_Disk_Latency.csv',\n",
    "    'Pod_User_CPU_Usage.csv',\n",
    "    'Pod_System_CPU_Usage.csv',\n",
    "    'Pod_Memory_Usage.csv',\n",
    "    'Pod_Memory_Failures.csv',\n",
    "    'Pod_Disk_Writes.csv',\n",
    "    'Pod_Disk_Reads.csv',\n",
    "    'Pod_Network_Received.csv',\n",
    "    'Pod_Network_Transmitted.csv',\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set query time\n",
    "# start_time = dt.datetime(2022, 5, 21, 1, 00, 00)+dt.timedelta(hours=8)\n",
    "# start_time = dt.datetime(2022, 5, 27, 19, 30, 00)+dt.timedelta(hours=8)\n",
    "start_time = dt.datetime(2022, 5, 29, 21, 10, 00)+dt.timedelta(hours=8)\n",
    "# start_time = end_time-dt.timedelta(hours=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(query_list)):\n",
    "    query_df = pd.DataFrame()\n",
    "    for t in range(1):\n",
    "        mstart_time = start_time+dt.timedelta(hours=1)*t\n",
    "        mend_time = mstart_time+dt.timedelta(hours=1)-dt.timedelta(seconds=1)\n",
    "        metric_data = prom.custom_query_range(\n",
    "            query_list[i],\n",
    "            start_time=mstart_time,\n",
    "            end_time=mend_time,\n",
    "            step=1\n",
    "        )\n",
    "\n",
    "        metric_df = pd.DataFrame(columns=['timestamp'])\n",
    "        for m in metric_data:\n",
    "            if query_info_df.metric[i] == 'None':\n",
    "                col_name = \"None\"\n",
    "            else:\n",
    "                if m['metric'] == {} and len(metric_data) > 1:\n",
    "                    continue\n",
    "                sub_metrics = query_info_df.metric[i].split(\"+\")\n",
    "                col_name = \"_\".join(\n",
    "                    list(map(lambda x: m['metric'][x], sub_metrics)))\n",
    "                col_name = col_name.replace('-', '_')\n",
    "            temp_df = pd.DataFrame(m['values'], columns=[\n",
    "                                   'timestamp', col_name])\n",
    "            metric_df = pd.merge(metric_df, temp_df,\n",
    "                                 on='timestamp', how='outer')\n",
    "\n",
    "        query_df = pd.concat([query_df, metric_df])\n",
    "\n",
    "    save_path = os.path.join(query_save_path, '_'.join(\n",
    "        (query_info_df.name[i]).split())+'.csv')\n",
    "    query_df.to_csv(save_path, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.DataFrame(columns=['timestamp'])\n",
    "# walk through directory\n",
    "for root, dirs, files in os.walk(query_save_path):\n",
    "    for file in files:\n",
    "        if file.endswith(\".csv\"):\n",
    "            if file not in dropped:\n",
    "                temp_df = pd.read_csv(os.path.join(root, file))\n",
    "                if temp_df.empty:\n",
    "                    continue\n",
    "                file_name = file.split('.')[0]\n",
    "                # rename temp_df columns\n",
    "                temp_df.columns = ['timestamp'] + list(\n",
    "                    map(lambda x: file_name+'__'+x, temp_df.columns[1:]))\n",
    "                combined_df = pd.merge(combined_df, temp_df,\n",
    "                                    on='timestamp', how='outer')\n",
    "combined_df.set_index('timestamp')\n",
    "# drop columns all nan\n",
    "combined_df.dropna(axis=1, how='all', inplace=True)\n",
    "combined_df.interpolate(inplace=True, limit=60, method='linear')\n",
    "# fill nan by 0\n",
    "combined_df.fillna(0, inplace=True)\n",
    "combined_df = combined_df.loc[:, (combined_df != 0).any(axis=0)]\n",
    "\n",
    "combined_df.to_csv(os.path.join(\n",
    "    dirname(dirname(os.getcwd())), \"output\", \"mysql\", 'test1_'+interval+\".csv\"), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jzlch\\AppData\\Local\\Temp/ipykernel_12364/1165669979.py:16: ConvergenceWarning: Number of distinct clusters (5) found smaller than n_clusters (10). Possibly due to duplicate points in X.\n",
      "  kmeans.fit(temp_df[col].to_numpy().reshape(-1, 1))\n",
      "C:\\Users\\jzlch\\AppData\\Local\\Temp/ipykernel_12364/1165669979.py:16: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (10). Possibly due to duplicate points in X.\n",
      "  kmeans.fit(temp_df[col].to_numpy().reshape(-1, 1))\n",
      "C:\\Users\\jzlch\\AppData\\Local\\Temp/ipykernel_12364/1165669979.py:16: ConvergenceWarning: Number of distinct clusters (7) found smaller than n_clusters (10). Possibly due to duplicate points in X.\n",
      "  kmeans.fit(temp_df[col].to_numpy().reshape(-1, 1))\n",
      "C:\\Users\\jzlch\\AppData\\Local\\Temp/ipykernel_12364/1165669979.py:16: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (10). Possibly due to duplicate points in X.\n",
      "  kmeans.fit(temp_df[col].to_numpy().reshape(-1, 1))\n",
      "C:\\Users\\jzlch\\AppData\\Local\\Temp/ipykernel_12364/1165669979.py:16: ConvergenceWarning: Number of distinct clusters (2) found smaller than n_clusters (10). Possibly due to duplicate points in X.\n",
      "  kmeans.fit(temp_df[col].to_numpy().reshape(-1, 1))\n",
      "C:\\Users\\jzlch\\AppData\\Local\\Temp/ipykernel_12364/1165669979.py:16: ConvergenceWarning: Number of distinct clusters (3) found smaller than n_clusters (10). Possibly due to duplicate points in X.\n",
      "  kmeans.fit(temp_df[col].to_numpy().reshape(-1, 1))\n",
      "C:\\Users\\jzlch\\AppData\\Local\\Temp/ipykernel_12364/1165669979.py:16: ConvergenceWarning: Number of distinct clusters (2) found smaller than n_clusters (10). Possibly due to duplicate points in X.\n",
      "  kmeans.fit(temp_df[col].to_numpy().reshape(-1, 1))\n"
     ]
    }
   ],
   "source": [
    "temp_df = combined_df.copy()\n",
    "temp_df.drop(['timestamp'], axis=1, inplace=True)\n",
    "# minus by mean\n",
    "temp_df = temp_df - temp_df.mean()\n",
    "blip_df = pd.DataFrame()\n",
    "# chaos_state_df = pd.read_csv(os.path.join(\n",
    "#     os.pardir, \"output\", 'chaos_state.csv'))\n",
    "# blip_df[\"chaos_state\"]=chaos_state_df[\"chaos_state\"]\n",
    "\n",
    "# discritize each single column by kmeans cluster\n",
    "for col in temp_df.columns:\n",
    "    # if col==\"chaos__event_type\":\n",
    "    #     blip_df[col]=temp_df[col]\n",
    "    #     continue\n",
    "    kmeans = KMeans(n_clusters=10, random_state=0)\n",
    "    kmeans.fit(temp_df[col].to_numpy().reshape(-1, 1))\n",
    "    blip_df[col] = kmeans.labels_\n",
    "\n",
    "\n",
    "blip_df.to_csv(os.path.join(dirname(dirname(os.getcwd())),\n",
    "               \"output\", \"mysql\", 'train_blip_'+interval+\".csv\"), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy combined_df\n",
    "notears_raw_df = combined_df.copy()\n",
    "# standard normalization\n",
    "notears_raw_df = (notears_raw_df - notears_raw_df.mean()) / \\\n",
    "    (notears_raw_df.std())\n",
    "notears_raw_df.dropna(axis=1, how='all', inplace=True)\n",
    "notears_raw_df.to_csv(os.path.join(os.pardir, \"output\",\n",
    "                      'notears_raw.csv'), index=False)\n",
    "\n",
    "notears_df = notears_raw_df.drop(['timestamp'], axis=1)\n",
    "notears_df.to_csv(os.path.join(os.pardir, \"output\",\n",
    "                  'notears.csv'), index=False, header=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "offline_df=pd.read_csv(r\"C:\\Users\\jzlch\\Documents\\Project\\TiDB_exp\\output\\mysql\\train_1m_plus.csv\")\n",
    "online_df=pd.read_csv(r\"C:\\Users\\jzlch\\Documents\\Project\\TiDB_exp\\output\\mysql\\test_1m.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jzlch\\AppData\\Local\\Temp/ipykernel_17156/1889268263.py:5: ConvergenceWarning: Number of distinct clusters (5) found smaller than n_clusters (10). Possibly due to duplicate points in X.\n",
      "  kmeans.fit(offline_df[col].to_numpy().reshape(-1, 1))\n",
      "C:\\Users\\jzlch\\AppData\\Local\\Temp/ipykernel_17156/1889268263.py:8: ConvergenceWarning: Number of distinct clusters (6) found smaller than n_clusters (10). Possibly due to duplicate points in X.\n",
      "  kmeans.fit(online_df[col].to_numpy().reshape(-1, 1))\n",
      "C:\\Users\\jzlch\\AppData\\Local\\Temp/ipykernel_17156/1889268263.py:5: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (10). Possibly due to duplicate points in X.\n",
      "  kmeans.fit(offline_df[col].to_numpy().reshape(-1, 1))\n",
      "C:\\Users\\jzlch\\AppData\\Local\\Temp/ipykernel_17156/1889268263.py:8: ConvergenceWarning: Number of distinct clusters (2) found smaller than n_clusters (10). Possibly due to duplicate points in X.\n",
      "  kmeans.fit(online_df[col].to_numpy().reshape(-1, 1))\n",
      "C:\\Users\\jzlch\\AppData\\Local\\Temp/ipykernel_17156/1889268263.py:5: ConvergenceWarning: Number of distinct clusters (7) found smaller than n_clusters (10). Possibly due to duplicate points in X.\n",
      "  kmeans.fit(offline_df[col].to_numpy().reshape(-1, 1))\n",
      "C:\\Users\\jzlch\\AppData\\Local\\Temp/ipykernel_17156/1889268263.py:5: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (10). Possibly due to duplicate points in X.\n",
      "  kmeans.fit(offline_df[col].to_numpy().reshape(-1, 1))\n",
      "C:\\Users\\jzlch\\AppData\\Local\\Temp/ipykernel_17156/1889268263.py:8: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (10). Possibly due to duplicate points in X.\n",
      "  kmeans.fit(online_df[col].to_numpy().reshape(-1, 1))\n",
      "C:\\Users\\jzlch\\AppData\\Local\\Temp/ipykernel_17156/1889268263.py:5: ConvergenceWarning: Number of distinct clusters (2) found smaller than n_clusters (10). Possibly due to duplicate points in X.\n",
      "  kmeans.fit(offline_df[col].to_numpy().reshape(-1, 1))\n",
      "C:\\Users\\jzlch\\AppData\\Local\\Temp/ipykernel_17156/1889268263.py:8: ConvergenceWarning: Number of distinct clusters (8) found smaller than n_clusters (10). Possibly due to duplicate points in X.\n",
      "  kmeans.fit(online_df[col].to_numpy().reshape(-1, 1))\n",
      "C:\\Users\\jzlch\\AppData\\Local\\Temp/ipykernel_17156/1889268263.py:5: ConvergenceWarning: Number of distinct clusters (3) found smaller than n_clusters (10). Possibly due to duplicate points in X.\n",
      "  kmeans.fit(offline_df[col].to_numpy().reshape(-1, 1))\n",
      "C:\\Users\\jzlch\\AppData\\Local\\Temp/ipykernel_17156/1889268263.py:5: ConvergenceWarning: Number of distinct clusters (2) found smaller than n_clusters (10). Possibly due to duplicate points in X.\n",
      "  kmeans.fit(offline_df[col].to_numpy().reshape(-1, 1))\n"
     ]
    }
   ],
   "source": [
    "blip_offline_df = pd.DataFrame()\n",
    "blip_online_df = pd.DataFrame()\n",
    "for col in offline_df.columns[1:]:\n",
    "    kmeans = KMeans(n_clusters=10, random_state=0)\n",
    "    kmeans.fit(offline_df[col].to_numpy().reshape(-1, 1))\n",
    "    blip_offline_df[col] = kmeans.labels_\n",
    "    if col in online_df.columns:\n",
    "        kmeans.fit(online_df[col].to_numpy().reshape(-1, 1))\n",
    "        blip_online_df[col] = kmeans.labels_\n",
    "    else:\n",
    "        blip_online_df[col] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "blip_offline_df.to_csv(os.path.join(dirname(dirname(os.getcwd())),\n",
    "               \"output\", \"mysql\", \"offline.csv\"), index=False)\n",
    "blip_online_df.to_csv(os.path.join(dirname(dirname(os.getcwd())),\n",
    "                \"output\", \"mysql\", \"online.csv\"), index=False)\n",
    "\n",
    "blip_offline_less_df=blip_offline_df.iloc[37800:,:]\n",
    "blip_offline_less_df.to_csv(os.path.join(dirname(dirname(os.getcwd())),\n",
    "               \"output\", \"mysql\", \"offline_withoutChaos.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(pd.isnull(new_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(r\"C:\\Users\\jzlch\\Documents\\Project\\TiDB_exp\\data\\mysql\\train\\1s\\MySQL_Client_Thread_Activity__Peak_Threads_Connected.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(x=df.columns[0], y=df.columns[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2a8dfe095fce2b5e88c64a2c3ee084c8e0e0d70b23e7b95b1cfb538be294c5c8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
