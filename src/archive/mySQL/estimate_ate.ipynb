{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cusum import *\n",
    "\n",
    "from os.path import (\n",
    "    abspath,\n",
    "    dirname,\n",
    ")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle as pkl\n",
    "from scipy import stats\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from pygraphviz import AGraph\n",
    "import warnings\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "warnings.filterwarnings(action='ignore', category=DataConversionWarning)\n",
    "warnings.filterwarnings(action='ignore', category=FutureWarning)\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "tf.config.set_visible_devices([], 'GPU')\n",
    "# sess = tf.compat.v1.Session(\n",
    "#     config=tf.compat.v1.ConfigProto(log_device_placement=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global variables\n",
    "model_file = \"/home/zjiae/Results/exp_data/mysql_plus\"\n",
    "graph_file = \"/home/zjiae/Project/TiDB_exp/inference/mysql/manual.txt\"\n",
    "# graph_file = \"/home/zjiae/Project/TiDB_exp/inference/mysql/blip_withoutCE.txt\"\n",
    "train_file = \"/home/zjiae/Project/TiDB_exp/inference/mysql/train_1m_plus.csv\"\n",
    "# normal_data_file = \"/home/zjiae/Project/TiDB_exp/inference/test_normal.csv\"\n",
    "# current_data_file = \"/home/zjiae/Project/TiDB_exp/inference/test_anomaly.csv\"\n",
    "test_file=\"/home/zjiae/Project/TiDB_exp/inference/mysql/test1_1m.csv\"\n",
    "\n",
    "train_df = pd.read_csv(train_file)\n",
    "test_df = pd.read_csv(test_file)\n",
    "\n",
    "nodes_list = train_df.columns.to_list()\n",
    "with open(graph_file, 'r') as fin:\n",
    "    origin_edges = fin.read().splitlines()\n",
    "\n",
    "\n",
    "dml_model_list = list(map(lambda x: x.split(\n",
    "    '.')[0], os.listdir(os.path.join(model_file, \"dml\"))))\n",
    "deepiv_model_list = os.listdir(os.path.join(model_file, \"deepiv\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_parents(node_name, edge_list):\n",
    "    parents = []\n",
    "    for edge in edge_list:\n",
    "        treatment, outcome = edge.strip(';').split(' -> ')\n",
    "        if node_name == outcome:\n",
    "            parents.append(treatment)\n",
    "    return parents\n",
    "\n",
    "\n",
    "def deepiv_ate(deepiv_model, X=None, T0=0, T1=1):\n",
    "    if np.ndim(T0) == 0:\n",
    "        T0 = np.repeat(T0, 1 if X is None else np.shape(X)[0])\n",
    "    if np.ndim(T1) == 0:\n",
    "        T1 = np.repeat(T1, 1 if X is None else np.shape(X)[0])\n",
    "    # if X is None:\n",
    "    #     print('No data provided. Returning NaN.')\n",
    "    #     return np.nan\n",
    "    return (deepiv_model.predict([T1, X]) - deepiv_model.predict([T0, X])).reshape(-1, 1)\n",
    "\n",
    "\n",
    "def for_estimation(node_name, value):\n",
    "    if train_df[node_name].std() == 0:\n",
    "        return value-value\n",
    "    else:\n",
    "        return (value - train_df[node_name].mean()) / train_df[node_name].std()\n",
    "    # return (value - train_df[node_name].min()) / (train_df[node_name].max() - train_df[node_name].min())\n",
    "\n",
    "\n",
    "def one_tailed_pValue(distribution, value):\n",
    "    if distribution is None:\n",
    "        return 0.5 if value == 0 else 0\n",
    "    cdf = distribution.cdf(value)\n",
    "    if cdf > 0.5:\n",
    "        return 1 - cdf\n",
    "    else:\n",
    "        return cdf\n",
    "\n",
    "\n",
    "def calc_ate(edge, t0, t1):\n",
    "    if edge in dml_model_list:\n",
    "        # read pickle file\n",
    "        with open(os.path.join(model_file, \"dml\", edge+\".pkl\"), 'rb') as fin:\n",
    "            dml_model_config = pkl.load(fin)\n",
    "\n",
    "        X_now = [(current_log.get(c) - train_df.get(c).mean()) / train_df.get(c).std()\n",
    "                 for c in dml_model_config['confounder']]\n",
    "        X_now = np.array(X_now).reshape(1, -1)\n",
    "\n",
    "        ate = dml_model_config['est'].ate(\n",
    "            X=X_now, T0=t0, T1=t1).mean()\n",
    "\n",
    "    elif edge in deepiv_model_list:\n",
    "        deepiv_model = keras.models.load_model(\n",
    "            os.path.join(model_file, \"deepiv\", edge))\n",
    "        # with open(os.path.join(model_file, \"deepiv\", edge, \"causal.pkl\"), 'rb') as fin:\n",
    "        #     deepiv_model_config = pkl.load(fin)\n",
    "\n",
    "        ate = deepiv_ate(deepiv_model, X=np.zeros((1, 1)), T0=t0,\n",
    "                         T1=t1).mean()\n",
    "    return ate\n",
    "\n",
    "\n",
    "# def final_effect(trace_list, treat_node, t0, t1):\n",
    "#     alt_effects = []\n",
    "#     for trace in trace_list:\n",
    "#         if trace[0] == treat_node:\n",
    "#             if trace[1] == start_node:\n",
    "#                 return calc_ate(trace[0]+'-'+trace[1], t0, t1)\n",
    "#             else:\n",
    "#                 effect = calc_ate(trace[0]+'-'+trace[1], t0, t1)\n",
    "#                 next_node = trace[1]\n",
    "#                 if current_log.get(next_node) is None:\n",
    "#                     alt_effects.append(0)\n",
    "#                 else:\n",
    "#                     t1_next = for_estimation(\n",
    "#                         next_node, current_log.get(next_node))\n",
    "#                     t0_next = t1_next-effect\n",
    "#                     alt_effects.append(final_effect(\n",
    "#                         trace_list, next_node, t0_next, t1_next))\n",
    "#     # print(treat_node,alt_effects)\n",
    "#     return sum(alt_effects)\n",
    "\n",
    "\n",
    "def counter_factual(DG, sorted_nodes, treat_node_idx):\n",
    "    treat_node = sorted_nodes[treat_node_idx]\n",
    "    effect_dict = {}\n",
    "\n",
    "    if treat_node in normal_df.columns:\n",
    "        t0 = for_estimation(treat_node, normal_df[treat_node].mean())\n",
    "    else:\n",
    "        t0 = for_estimation(treat_node, 0)\n",
    "\n",
    "    t1 = for_estimation(treat_node, current_log[treat_node])\n",
    "    for s in DG.successors(treat_node):\n",
    "        edge = treat_node+'-'+s\n",
    "        effect_dict[edge] = calc_ate(edge, t0, t1)\n",
    "\n",
    "    for i in range(treat_node_idx+1, len(sorted_nodes)-1):\n",
    "        node = sorted_nodes[i]\n",
    "        if current_log.get(node) is None:\n",
    "            for s in DG.successors(node):\n",
    "                edge = node+'-'+s\n",
    "                effect_dict[edge] = 0\n",
    "        else:\n",
    "            temp_t1 = for_estimation(node, current_log[node])\n",
    "\n",
    "            temp_effect = 0\n",
    "            for p in DG.predecessors(node):\n",
    "                edge = p+'-'+node\n",
    "                if effect_dict.get(edge) is not None:\n",
    "                    temp_effect += effect_dict[edge]\n",
    "            temp_t0 = temp_t1-temp_effect\n",
    "\n",
    "            for s in DG.successors(node):\n",
    "                edge = node+'-'+s\n",
    "                effect_dict[edge] = calc_ate(edge, temp_t0, temp_t1)\n",
    "    ret_effect = 0\n",
    "    for p in DG.predecessors(sorted_nodes[-1]):\n",
    "        edge = p+'-'+sorted_nodes[-1]\n",
    "        if effect_dict.get(edge) is not None:\n",
    "            ret_effect += effect_dict[edge]\n",
    "        else:\n",
    "            ret_effect += 0\n",
    "    return ret_effect\n",
    "\n",
    "\n",
    "def dfs_cause(DG, node, visited):\n",
    "    \"\"\"\n",
    "    This function reproduce CauseInfer\n",
    "    \"\"\"\n",
    "    visited.add(node)\n",
    "    isAnomaly = False\n",
    "    if node in current_log:\n",
    "        x = test_df[start_node][(test_df[\"timestamp\"] >= observed_time-180) &\n",
    "                                (test_df[\"timestamp\"] <= observed_time+120)]\n",
    "        x = (x-x.min())/(x.max()-x.min())\n",
    "        std = x.std()\n",
    "        for i in range(3, 6):\n",
    "            res, _, _, _ = detect_cusum(x[0:60], i/10, 0, True, False)\n",
    "            if res.shape[0] == 0:\n",
    "                res_1, _, _, _ = detect_cusum(x[120:180], i/10, 0, True, False)\n",
    "                if res_1.shape[0] != 0:\n",
    "                    isAnomaly = True\n",
    "                break\n",
    "\n",
    "        # detect_cusum(x[s:s+60], 0.3, 0, True, True)\n",
    "        # ref_data = train_df[node].to_numpy()\n",
    "        # ref_data = for_estimation(node, ref_data)\n",
    "        # if ref_data.std() == 0:\n",
    "        #     temp_PDF = 0\n",
    "        # else:\n",
    "        #     temp_distribution = stats.gaussian_kde(ref_data)\n",
    "        #     temp_value = for_estimation(node, current_log[node])\n",
    "        #     temp_PDF = temp_distribution.pdf(temp_value)[0]\n",
    "    # if temp_PDF < threshold:\n",
    "    if isAnomaly:\n",
    "        for predecessor in DG.predecessors(node):\n",
    "            if predecessor not in visited:\n",
    "                visited.union(dfs_cause(DG, predecessor, visited))\n",
    "    return visited\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_node = \"MySQL_Query_Duration__None\"\n",
    "\n",
    "observed_time=1653859785\n",
    "\n",
    "# normal_start=1653679800\n",
    "# normal_stop=1653681600\n",
    "normal_start=1653858600\n",
    "normal_stop=1653859500\n",
    "\n",
    "# slice normal_df from normal_star to normal_stop\n",
    "normal_df = test_df.loc[(test_df['timestamp']>=normal_start) & (test_df['timestamp']<normal_stop)]\n",
    "\n",
    "current_log = test_df[test_df[\"timestamp\"]\n",
    "                         == observed_time].to_dict('list')\n",
    "current_log = {k: v[0] for k, v in current_log.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average value of normal: 0.6645449529196077\n",
      "Anomaly value: 0.4402866924732841\n",
      "fact_PDF: [1.26705563]\n"
     ]
    }
   ],
   "source": [
    "if start_node not in nodes_list:\n",
    "    print(\"Node not found\")\n",
    "\n",
    "print(f\"Average value of normal: {normal_df[start_node].mean()}\")\n",
    "print(f\"Anomaly value: {current_log[start_node]}\")\n",
    "start_data = train_df[start_node].to_numpy()\n",
    "# start_data = (start_data-start_data.min())/(start_data.max()-start_data.min())\n",
    "start_data=for_estimation(start_node,start_data)\n",
    "# params = stats.norm.fit(start_data)\n",
    "# node_distribution = stats.norm(*params)\n",
    "node_distribution = stats.gaussian_kde(start_data)\n",
    "fact_start = for_estimation(start_node, current_log[start_node])\n",
    "fact_PDF = node_distribution.pdf(fact_start)\n",
    "print(\"fact_PDF:\", fact_PDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_point = 0\n",
    "visit_list = [start_node, ]\n",
    "trace_list = []\n",
    "causes = {}\n",
    "\n",
    "while(True):\n",
    "    if node_point >= len(visit_list):\n",
    "        # print(\"No more node. Done!\")\n",
    "        break\n",
    "    # print(\"Try to find the parent of {}\".format(visit_list[node_point]))\n",
    "    parents = find_parents(visit_list[node_point], origin_edges)\n",
    "    if len(parents) > 0:\n",
    "        for parent in parents:\n",
    "            if parent not in visit_list:\n",
    "                visit_list.append(parent)\n",
    "            trace_list.append((parent, visit_list[node_point]))\n",
    "    node_point += 1\n",
    "\n",
    "DG=nx.DiGraph()\n",
    "DG.add_edges_from(trace_list)\n",
    "sorted_nodes=list(nx.topological_sort(DG))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "cause_infer=dfs_cause(DG, start_node, set())\n",
    "# cause_infer=list(cause_infer)\n",
    "# print items in sorted_nodes with index\n",
    "for i in range(len(sorted_nodes)):\n",
    "    if sorted_nodes[i] in cause_infer:\n",
    "        print(sorted_nodes[i])\n",
    "    if i >=5:\n",
    "        break\n",
    "\n",
    "# for i in range(len(cause_infer)):\n",
    "#     print(i,cause_infer[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/18 has been done\n",
      "10/18 has been done\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(sorted_nodes)-1):\n",
    "    effect = counter_factual(DG, sorted_nodes, i)\n",
    "    counter_start = fact_start-effect\n",
    "    counter_PDF = node_distribution.pdf(counter_start)\n",
    "    causes[sorted_nodes[i]] = (counter_PDF-fact_PDF, effect)\n",
    "    if i % 10 == 0:\n",
    "        # break\n",
    "        print(f\"{i}/{len(sorted_nodes)} has been done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0.08213282]), -0.006238382309675217) System_Memory_Distribution__Free__None\n",
      "(array([0.]), 0.0) MySQL_Query_Cache_Memory__Free_Memory__None\n",
      "(array([0.]), 0.0) System_Swap_Activity__Swap_In_Reads__None\n",
      "(array([0.]), 0.0) MySQL_Connections__Max_Used_Connections__None\n",
      "(array([-0.13422876]), 0.01066964864730835) MySQL_Network_Usage_Hourly__Sent__None\n",
      "(array([-0.14258019]), 0.011356402188539505) System_IO_Activity__Page_In__None\n",
      "(array([-0.21049012]), 0.017057016491889954) MySQL_Network_Usage_Hourly__Received__None\n",
      "(array([-0.71579642]), 0.07052215933799744) MySQL_Handlers__write\n",
      "(array([-1.26636662]), -1.687682867050171) System_IO_Activity__Page_Out__None\n",
      "(array([-1.26705563]), -54.988468170166016) System_CPU_Usage_Load__user\n",
      "(array([-1.26705563]), 3.1834304332733154) MySQL_Transaction_Handlers__commit\n",
      "(array([-1.26705563]), 3.0300304889678955) MySQL_Handlers__external_lock\n",
      "(array([-1.26705563]), 2.9995107650756836) Current_QPS__None\n",
      "(array([-1.26705563]), 2.9692225456237793) MySQL_Network_Traffic__Inbound__None\n",
      "(array([-1.26705563]), 2.702477216720581) MySQL_Network_Traffic__Outbound__None\n",
      "(array([-1.26705563]), -4.22147274017334) System_CPU_Usage_Load__Max_Core_Utilization__None\n",
      "(array([-1.26705563]), -4.210237503051758) System_CPU_Usage_Load__Load_1m__None\n"
     ]
    }
   ],
   "source": [
    "sorted_causes=dict(sorted(causes.items(), key=lambda item: item[1][0], reverse=True))\n",
    "for k,v in sorted_causes.items():\n",
    "    print(v,k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_pdf = dict()\n",
    "sorted_anomaly_nodes = []\n",
    "for node in nodes_list:\n",
    "    if node == \"timestamp\":\n",
    "        continue\n",
    "    if node in current_log:\n",
    "        ref_data = train_df[node].to_numpy()\n",
    "        ref_data = for_estimation(node, ref_data)\n",
    "        if ref_data.std() == 0:\n",
    "            nodes_pdf[node] = 0\n",
    "        else:\n",
    "            temp_distribution = stats.gaussian_kde(ref_data)\n",
    "            temp_value = for_estimation(node, current_log[node])\n",
    "            temp_PDF = temp_distribution.pdf(temp_value)\n",
    "            nodes_pdf[node] = temp_PDF[0]\n",
    "\n",
    "sorted_anomaly_nodes = sorted(\n",
    "    nodes_pdf.items(), key=lambda x: x[1])\n",
    "for i in range(10):\n",
    "    print(sorted_anomaly_nodes[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for trace in trace_list:\n",
    "    print(f\"{trace[0]} -> {trace[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax=plt.hist(start_data, density=True, bins=50)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "npts_sample = int(1e4)\n",
    "x = np.linspace(start_data.min(), start_data.max(), npts_sample)\n",
    "kde_pdf = node_distribution.evaluate(x)\n",
    "ax.plot(x, kde_pdf)\n",
    "\n",
    "# x=node_distribution.rvs(size=10000)\n",
    "# ax.hist(x, density=True, bins=50)\n",
    "# plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ade24f262e332a0eb6eb5ffe11949e4fa607f424ef2355f5c2ecb86f04d50e15"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
